import torch
from transformers import (
    CLIPSegProcessor,
    CLIPSegForImageSegmentation,
    AutoProcessor,
    AutoModelForZeroShotObjectDetection,
)

import sys
import huggingface_hub as _hf
from huggingface_hub import hf_hub_download as _hf_hub_download

if not hasattr(_hf, "cached_download"):
    def cached_download(*args, **kwargs):
        return _hf_hub_download(*args, **kwargs)

    _hf.cached_download = cached_download
    sys.modules["huggingface_hub"] = _hf
# ========================================================================

from diffusers import StableDiffusionInpaintPipeline, DDPMScheduler
from diffusers.utils import load_image
from PIL import Image, ImageDraw
import numpy as np

# ==========================
# 0. åƒæ•¸è¨­å®š
# ==========================
IMG_PATH = "Screenshot 2025-11-27 200322.png"           # ä½ çš„ç°¡å ±æˆªåœ–
BOX_TO_FIND = "table" # Grounding DINO æœå°‹æ–‡å­—
TEXT_TO_FIND = "table"   # CLIPSeg æœå°‹æ–‡å­—
PROMPT = "change the table for a dog"  # PowerPaint è¦ç•«ä»€éº¼
DEVICE = "cuda"

GDINO_MODEL_ID = "IDEA-Research/grounding-dino-tiny"  # Grounding DINO æ¨¡å‹

print(f"ğŸš€ åˆå§‹åŒ–... æ­£åœ¨è¼‰å…¥ CLIPSegã€Grounding DINO å’Œ PowerPaint...")

# ------------------------------------------------
# è¼‰å…¥ CLIPSeg (è² è²¬ segmentation)
# ------------------------------------------------
clipseg_processor = CLIPSegProcessor.from_pretrained("CIDAS/clipseg-rd64-refined")
clipseg_model = CLIPSegForImageSegmentation.from_pretrained(
    "CIDAS/clipseg-rd64-refined"
).to(DEVICE)

# ------------------------------------------------
# è¼‰å…¥ Grounding DINO (è² è²¬ bounding box)
# ------------------------------------------------
gdino_processor = AutoProcessor.from_pretrained(GDINO_MODEL_ID)
gdino_model = AutoModelForZeroShotObjectDetection.from_pretrained(
    GDINO_MODEL_ID
).to(DEVICE)

# ------------------------------------------------
# è¼‰å…¥ PowerPaint (è² è²¬ä¿®åœ– / inpaint)
# ------------------------------------------------
pipe = StableDiffusionInpaintPipeline.from_pretrained(
    "Sanster/PowerPaint-V1-stable-diffusion-inpainting",
    torch_dtype=torch.float16,
    safety_checker=None,
).to(DEVICE)

# ==========================
# 1. è®€åœ–
# ==========================
original_image = load_image(IMG_PATH).convert("RGB")
W, H = original_image.size
print(f"ğŸ“¸ è®€å–åœ–ç‰‡: {W}x{H}")

# ==========================
# 2. Grounding DINO æ‰¾ bounding box
# ==========================
print(f"ğŸ§­ Grounding DINO æ­£åœ¨å°‹æ‰¾ '{BOX_TO_FIND}' çš„å€åŸŸ (bounding box)...")

# âœ… é€™è£¡æ”¹æˆå­—ä¸²ï¼ˆæˆ– List[str]ï¼‰ï¼Œä¸è¦ç”¨ List[List[str]]
gdino_text = BOX_TO_FIND

gdino_inputs = gdino_processor(
    images=original_image,
    text=gdino_text,
    return_tensors="pt",
).to(DEVICE)

with torch.no_grad():
    gdino_outputs = gdino_model(**gdino_inputs)

# æŠŠ raw output è½‰æˆå¯¦éš›åº§æ¨™ (x0, y0, x1, y1)
gdino_results = gdino_processor.post_process_grounded_object_detection(
    outputs=gdino_outputs,
    input_ids=gdino_inputs.input_ids,
    box_threshold=0.09,      # box confidence é–€æª»
    text_threshold=0.05,     # æ–‡å­—åŒ¹é…é–€æª»
    target_sizes=[(H, W)],   # (height, width)
)

gdino_res = gdino_results[0]
boxes = gdino_res["boxes"]   # tensor [num_boxes, 4]
scores = gdino_res["scores"] # tensor [num_boxes]

if boxes.shape[0] == 0:
    raise RuntimeError(
        f"[Grounding DINO] æ²’æœ‰æ‰¾åˆ°å’Œ '{BOX_TO_FIND}' å°æ‡‰çš„ç‰©ä»¶ï¼Œ"
        "å¯ä»¥è©¦è©¦é™ä½ threshold æˆ–æ›ä¸€å€‹æè¿°ã€‚"
    )

# å–åˆ†æ•¸æœ€é«˜çš„é‚£å€‹ box
best_idx = scores.argmax().item()
best_box = boxes[best_idx].tolist()
x0, y0, x1, y1 = [int(v) for v in best_box]
print(f"âœ… Grounding DINO æœ€ä½³æ¡†: ({x0}, {y0}) -> ({x1}, {y1}), score = {scores[best_idx].item():.3f}")

# è¼¸å‡ºç•«æœ‰ bounding box çš„ debug åœ–
debug_box_img = original_image.copy()
draw = ImageDraw.Draw(debug_box_img)
draw.rectangle([x0, y0, x1, y1], outline="red", width=3)
draw.text((x0, max(0, y0 - 15)), BOX_TO_FIND, fill="red")
debug_box_img.save("debug_groundingdino_box.png")
print("ğŸ–¼ å·²è¼¸å‡º Grounding DINO çµæœåœ–ï¼šdebug_groundingdino_box.png")

# ==========================
# 3. åœ¨ bounding box è£¡ç”¨ CLIPSeg åš mask
# ==========================
print(f"ğŸ¯ CLIPSeg åœ¨ box ä¸­å°‹æ‰¾ '{TEXT_TO_FIND}' çš„ç²¾ç´° mask...")

# è£å‡º Grounding DINO æä¾›çš„å­åœ–
crop = original_image.crop((x0, y0, x1, y1))
box_w, box_h = crop.size

clipseg_inputs = clipseg_processor(
    text=[TEXT_TO_FIND],
    images=[crop],
    return_tensors="pt",
).to(DEVICE)

with torch.no_grad():
    clipseg_outputs = clipseg_model(**clipseg_inputs)

# logits å½¢ç‹€ç‚º (batch, H', W') â†’ å–ç¬¬ 0 å¼µ
preds = torch.sigmoid(clipseg_outputs.logits)[0]  # (h', w')
mask_crop = preds.cpu().numpy()

# äºŒå€¼åŒ–
mask_crop = (mask_crop > 0.4).astype(np.uint8) * 255  # 0 or 255

# è½‰æˆèˆ‡ bounding box ä¸€æ¨£å¤§å°
mask_crop_img = Image.fromarray(mask_crop).resize(
    (box_w, box_h), resample=Image.NEAREST
)

# æŠŠ box å…§çš„ mask è²¼å›æ•´å¼µåœ–ä¸Š
full_mask_np = np.zeros((H, W), dtype=np.uint8)
full_mask_np[y0:y1, x0:x1] = np.array(mask_crop_img)

# [é˜²ç¦¦æ©Ÿåˆ¶] å¼·åˆ¶æŠŠå·¦é‚Š 30% å¡—é»‘ï¼Œä¿è­·ç°¡å ±å·¦å´æ–‡å­—
full_mask_np[:, : int(W * 0.3)] = 0

mask_image = Image.fromarray(full_mask_np)

# è¼¸å‡º CLIPSeg å¾—åˆ°çš„ mask debug åœ–
mask_image.save("debug_clipseg_mask.png")
print("ğŸ–¼ å·²è¼¸å‡º CLIPSeg mask åœ–ï¼šdebug_clipseg_mask.png")

# ==========================
# 4. åªå° bounding box å€åŸŸåšç¸®æ”¾ï¼Œä¸Ÿçµ¦ PowerPaint
# ==========================
# é€™è£¡ç”¨çš„æ˜¯ DINO æ‰¾åˆ°çš„ cropï¼Œè€Œä¸æ˜¯æ•´å¼µ original_image
patch_image = original_image.crop((x0, y0, x1, y1))      # bbox è£¡çš„åŸåœ–
patch_mask = mask_image.crop((x0, y0, x1, y1))          # bbox è£¡çš„ mask
box_w, box_h = patch_image.size

process_size = (512, 512)
input_image = patch_image.resize(process_size, resample=Image.LANCZOS)
input_mask = patch_mask.resize(process_size, resample=Image.NEAREST)

# ==========================
# 5. PowerPaint æ¨è«–ï¼ˆåªçœ‹ bbox é€™å¡Šï¼‰
# ==========================
print(f"ğŸ¨ PowerPaint æ­£åœ¨ç¹ªè£½: '{PROMPT}'ï¼ˆåªåœ¨ bounding box å€åŸŸï¼‰...")
output_small = pipe(
    prompt=PROMPT,
    image=input_image,
    mask_image=input_mask,
    negative_prompt="photorealistic, text, watermark, bad quality, blurry",
    num_inference_steps=50,
    strength=0.99,      # æ¥è¿‘ 1.0 ä»£è¡¨å®Œå…¨é‡ç¹ª Mask å€åŸŸ
    guidance_scale=12.5 # PowerPaint å»ºè­°ç”¨é«˜ä¸€é»çš„å¼•å°å€¼
).images[0]

# é‚„åŸå› bounding box åŸå°ºå¯¸
output_patch = output_small.resize((box_w, box_h), resample=Image.LANCZOS)

# åªåœ¨ bbox è£¡ã€ä¸” mask ç‚ºç™½è‰²çš„å€åŸŸåšæ›¿æ›ï¼Œå…¶é¤˜ä¿æŒåŸ crop
bbox_result = Image.composite(output_patch, patch_image, patch_mask)

# ==========================
# 6. æŠŠè™•ç†å¥½çš„ bbox è²¼å›æ•´å¼µåœ–
# ==========================
print("ğŸ”§ åˆæˆå›é«˜è§£æåº¦åŸåœ–ï¼ˆåƒ…æ›¿æ› bounding box å€åŸŸï¼‰...")
final_image = original_image.copy()
final_image.paste(bbox_result, (x0, y0))

final_image.save("final_result.png")
print("ğŸ‰ å¤§åŠŸå‘Šæˆï¼è«‹æŸ¥çœ‹ï¼šfinal_result.pngã€debug_groundingdino_box.pngã€debug_clipseg_mask.png")

